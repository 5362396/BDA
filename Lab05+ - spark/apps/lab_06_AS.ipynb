{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e62d0add040d5410",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "os.environ['SPARK_NAME'] = \"/opt/spark\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS'] = 'lab'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = '/opt/spark/work-dir/venv/bin/python3'\n",
    "os.environ['PYSPARK_PYTHON'] = '/opt/spark/work-dir/venv/bin/python3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "597ea0da94b20dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/16 16:40:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/11/16 16:40:28 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://aa6d41e84e3a:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Create-DataFrame</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[2] appName=Create-DataFrame>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import desc, sum, count\n",
    "\n",
    "\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"local[2]\")\\\n",
    "        .appName(\"Create-DataFrame\")\\\n",
    "        .config(\"spark.memory.offHeap.enabled\",\"true\")\\\n",
    "        .config(\"spark.memory.offHeap.size\",\"4g\")\\\n",
    "        .getOrCreate()\n",
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029eec6f-6845-4f90-b60f-faae4c5902d7",
   "metadata": {},
   "source": [
    "**Zadanie 1**  \n",
    "Na zbiorze danych '_Recipe Reviews ..._' wykonaj:  \n",
    "1.1  Zmień nazwę pierwszej kolumny z `_c0` na `id`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a080d076c3f438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- recipe_number: string (nullable = true)\n",
      " |-- recipe_code: string (nullable = true)\n",
      " |-- recipe_name: string (nullable = true)\n",
      " |-- comment_id: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- user_name: string (nullable = true)\n",
      " |-- user_reputation: string (nullable = true)\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- reply_count: string (nullable = true)\n",
      " |-- thumbs_up: string (nullable = true)\n",
      " |-- thumbs_down: string (nullable = true)\n",
      " |-- stars: string (nullable = true)\n",
      " |-- best_score: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "df_reviews = spark.read.csv('./data/Recipe Reviews and User Feedback Dataset.csv', header=True, sep=\",\")\n",
    "df_reviews.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9430ca3657156ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- recipe_number: string (nullable = true)\n",
      " |-- recipe_code: string (nullable = true)\n",
      " |-- recipe_name: string (nullable = true)\n",
      " |-- comment_id: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- user_name: string (nullable = true)\n",
      " |-- user_reputation: string (nullable = true)\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- reply_count: string (nullable = true)\n",
      " |-- thumbs_up: string (nullable = true)\n",
      " |-- thumbs_down: string (nullable = true)\n",
      " |-- stars: string (nullable = true)\n",
      " |-- best_score: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_reviews = df_reviews.withColumnRenamed('_c0', 'id')\n",
    "df_reviews.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2617d6dca51423",
   "metadata": {},
   "source": [
    "1.2  Wyświetl 10 najwyższych wartości w kolumnie `reply_count`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bb7b9bd4d72222f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 najwyższych wartości w kolumnie reply_count:\n",
      "+-----------+\n",
      "|reply_count|\n",
      "+-----------+\n",
      "|          3|\n",
      "|          3|\n",
      "|          3|\n",
      "|          3|\n",
      "|          3|\n",
      "|          3|\n",
      "|          2|\n",
      "|          2|\n",
      "|          2|\n",
      "|          2|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_reply_count = df_reviews.select('reply_count').orderBy(desc('reply_count')).limit(10)\n",
    "print('10 najwyższych wartości w kolumnie reply_count:')\n",
    "top_reply_count.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a931e128b9d33a43",
   "metadata": {},
   "source": [
    "1.3  Wyświetl 10 najwyższych sum wartości w kolumnie `best_score` dla każdego przepisu (grupowanie).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6b6e79ca66b109d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 najwyższych sum wartości w kolumnie best_score:\n",
      "+--------------------+----------------+\n",
      "|         recipe_name|total_best_score|\n",
      "+--------------------+----------------+\n",
      "|   Cheeseburger Soup|         98863.0|\n",
      "|  Creamy White Chili|         85497.0|\n",
      "|Amish Breakfast C...|         64880.0|\n",
      "|Best Ever Banana ...|         64247.0|\n",
      "|Favorite Chicken ...|         60755.0|\n",
      "|Basic Homemade Bread|         59867.0|\n",
      "|Flavorful Chicken...|         59195.0|\n",
      "|Zucchini Pizza Ca...|         54032.0|\n",
      "|Enchilada Casser-...|         51975.0|\n",
      "|    Cauliflower Soup|         47905.0|\n",
      "+--------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_best_score = df_reviews.groupBy('recipe_name') \\\n",
    "    .agg(sum('best_score').alias('total_best_score')) \\\n",
    "    .orderBy(desc('total_best_score')) \\\n",
    "    .limit(10)\n",
    "print('10 najwyższych sum wartości w kolumnie best_score:')\n",
    "top_best_score.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c62e4268f2635db",
   "metadata": {},
   "source": [
    "1.4  Które 10 przepisów miało najwięcej komentarzy?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a983fdf8c44f75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 przepisów z największą liczbą komentarzy:\n",
      "+--------------------+--------------+\n",
      "|         recipe_name|total_comments|\n",
      "+--------------------+--------------+\n",
      "|   Cheeseburger Soup|           725|\n",
      "|  Creamy White Chili|           654|\n",
      "|Best Ever Banana ...|           509|\n",
      "|Enchilada Casser-...|           421|\n",
      "|Basic Homemade Bread|           397|\n",
      "|Favorite Chicken ...|           395|\n",
      "|Flavorful Chicken...|           368|\n",
      "|Amish Breakfast C...|           338|\n",
      "|Zucchini Pizza Ca...|           332|\n",
      "|    Cauliflower Soup|           324|\n",
      "+--------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_comments = df_reviews.groupBy('recipe_name') \\\n",
    "    .agg(count('comment_id').alias('total_comments')) \\\n",
    "    .orderBy(desc('total_comments')) \\\n",
    "    .limit(10)\n",
    "print('10 przepisów z największą liczbą komentarzy:')\n",
    "top_comments.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a89c86579724a7e",
   "metadata": {},
   "source": [
    "1.5  Wyświetl rozkład wartości w kolumnie `stars`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87b2b9239ed7ee9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rozkład wartości w kolumnie stars:\n",
      "+-----+-----+\n",
      "|stars|count|\n",
      "+-----+-----+\n",
      "|    5|13829|\n",
      "|    4| 1655|\n",
      "|    3|  490|\n",
      "|    2|  232|\n",
      "|    1|  280|\n",
      "|    0| 1696|\n",
      "| NULL|   86|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stars_distribution = df_reviews.groupBy('stars').count().orderBy(desc('stars'))\n",
    "print('Rozkład wartości w kolumnie stars:')\n",
    "stars_distribution.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927b11f8-7c2a-436f-9c56-3999a471a1d3",
   "metadata": {},
   "source": [
    "**Zadanie 2**  \n",
    "Wczytaj zbiór danych `employee` nakazując Sparkowi wywnioskowanie bardziej optymalnych typów danych niż domyślny typ `string`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fa612fe9f597e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:================================================>         (5 + 1) / 6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      "\n",
      "+---+----------+----------+---+-------+\n",
      "| id| firstname|  lastname|age| salary|\n",
      "+---+----------+----------+---+-------+\n",
      "|  1|  Wojciech|Malinowski| 33|9301.66|\n",
      "|  2|      Adam|     Pysla| 38|8202.74|\n",
      "|  3|Aleksandra|  Barański| 37|7149.76|\n",
      "|  4|      Adam|    Szczaw| 35|7585.43|\n",
      "|  5|Aleksandra|  Kowalski| 46|7260.76|\n",
      "+---+----------+----------+---+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "employee_df = spark.read.csv('data/employee.csv', header=True, inferSchema=True)\n",
    "\n",
    "employee_df.printSchema()\n",
    "employee_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613d9963-d2e8-499d-a1ae-ecee4fb450d8",
   "metadata": {},
   "source": [
    "**Zadanie 3**  \n",
    "Jaki jest czas wykonania operacji `df.filter(df[\"salary\"] > 10000).count()` tym razem przy numerycznym typie kolumny `salary`? Jest jakaś różnica?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46321c70fd832e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:================================================>         (5 + 1) / 6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.71 ms, sys: 0 ns, total: 9.71 ms\n",
      "Wall time: 8.85 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "314703"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "employee_df.filter(employee_df['salary'] > 10000).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93db8fd6c00bef9",
   "metadata": {},
   "source": [
    "Czas operacji wydłużył się o około sekundę."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c6a85c-671b-4666-b6c1-8af05ad0de66",
   "metadata": {},
   "source": [
    "**Zadanie 4**  \n",
    "Wykorzystując przykład z dokumentacji klasy `Bucketizer` (https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.Bucketizer.html) podziel dane w kolumnie `age` zbioru `employee` na buckety co 10 lat (10-19, 20-29, ..., 60-69) i wyświetl te dane dla 20 pierwszych wierzy w formie surowej oraz całość grupując po bucketach i licząc ile osób znalazło się w każdym z nich."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "817bdde8c74f0986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dane surowe z bucketyzowaną kolumną:\n",
      "+---+----------+\n",
      "|age|age_bucket|\n",
      "+---+----------+\n",
      "| 33|       2.0|\n",
      "| 38|       2.0|\n",
      "| 37|       2.0|\n",
      "| 35|       2.0|\n",
      "| 46|       3.0|\n",
      "| 64|       5.0|\n",
      "| 18|       0.0|\n",
      "| 26|       1.0|\n",
      "| 43|       3.0|\n",
      "| 63|       5.0|\n",
      "| 18|       0.0|\n",
      "| 68|       5.0|\n",
      "| 34|       2.0|\n",
      "| 64|       5.0|\n",
      "| 25|       1.0|\n",
      "| 46|       3.0|\n",
      "| 38|       2.0|\n",
      "| 53|       4.0|\n",
      "| 59|       4.0|\n",
      "| 25|       1.0|\n",
      "+---+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Liczba osób w każdym buckecie:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:================================================>         (5 + 1) / 6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|age_bucket|  count|\n",
      "+----------+-------+\n",
      "|       0.0| 783516|\n",
      "|       1.0|3919857|\n",
      "|       2.0|3921865|\n",
      "|       3.0|3921239|\n",
      "|       4.0|3924423|\n",
      "|       5.0|3529100|\n",
      "+----------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "splits = [10, 20, 30, 40, 50, 60, 70]\n",
    "\n",
    "bucketizer = Bucketizer(\n",
    "    splits=splits,\n",
    "    inputCol='age',\n",
    "    outputCol='age_bucket'\n",
    ")\n",
    "\n",
    "bucketed_df = bucketizer.transform(employee_df)\n",
    "\n",
    "print('Dane surowe z bucketyzowaną kolumną:')\n",
    "bucketed_df.select('age', 'age_bucket').show(20)\n",
    "\n",
    "grouped_buckets = bucketed_df.groupBy('age_bucket').agg(count('*').alias('count'))\n",
    "\n",
    "print('Liczba osób w każdym buckecie:')\n",
    "grouped_buckets.orderBy('age_bucket').show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
